{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "# os.system('pip install iterative-stratification==0.1.7')\n",
    "# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset, Features, Value\n",
    "\n",
    "\n",
    "os.system('pip install -q transformers')\n",
    "os.system('pip install -q tokenizers')\n",
    "import tokenizers\n",
    "import transformers\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn import metrics\n",
    "# from src.machine_learning_util import set_seed, set_device, init_logger, AverageMeter, to_pickle, unpickle, asMinutes, timeSince"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    EXP_ID = '024'\n",
    "    apex = True\n",
    "    model ='microsoft/deberta-v3-large' # 'microsoft/deberta-large' # 'microsoft/deberta-v3-base' #'microsoft/deberta-v3-large' \n",
    "    seed = 2022 # 42 # 71\n",
    "    n_splits = 4\n",
    "    max_len = 640 + 2 # 1429 # 1024 # 512\n",
    "    dropout = 0\n",
    "    target_cols = \"label\"\n",
    "    target_size = None\n",
    "    n_accumulate=1\n",
    "    print_freq = 100\n",
    "    eval_freq = 780 * 2 # 390 # 170\n",
    "    min_lr=1e-6\n",
    "    scheduler = 'cosine'\n",
    "    batch_size = 12 # 2 # 4\n",
    "    num_workers = 0 #3\n",
    "    lr = 5e-6 # 3e-6\n",
    "    weigth_decay = 0.01\n",
    "    epochs = 3\n",
    "    n_fold = 4\n",
    "    trn_fold = [i for i in range(n_fold)]\n",
    "    train = True\n",
    "    num_warmup_steps = 0\n",
    "    num_cycles=0.5\n",
    "    debug = False\n",
    "    freezing = True\n",
    "    gradient_checkpoint = True\n",
    "    reinit_layers = 4 # 3\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    max_norm = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "           \n",
    "seed_everything(CFG.seed)\n",
    "# seed_everything(seed=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_report_file_path = \"./NER_Dataset/private_data\" #巡迴課程 val 資料集\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=f\"{test_report_file_path}/opendid_test.tsv\", delimiter='\\t',\n",
    "                       features = Features({\n",
    "                              'fid': Value('string'), 'idx': Value('int64'),\n",
    "                              'text': Value('string')}),\n",
    "                              column_names=['fid', 'idx', 'text'], keep_default_na=False)\n",
    "\n",
    "test_list = list(dataset[\"train\"])\n",
    "\n",
    "test_df = pd.DataFrame.from_dict(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name_list = [\"PATIENT\",\"DOCTOR\",\"USERNAME\",\"PROFESSION\",\"ROOM\",\"DEPARTMENT\",\"HOSPITAL\",\n",
    "              \"ORGANIZATION\",\"STREET\",\"CITY\",\"STATE\",\"COUNTRY\",\"ZIP\",\"LOCATION-OTHER\",\n",
    "              \"AGE\",\"DATE\",\"TIME\",\"DURATION\",\"SET\",\"PHONE\",\"FAX\",\"EMAIL\",\"URL\",\"IPADDR\",\n",
    "              \"SSN\",\"MEDICALRECORD\",\"HEALTHPLAN\",\"ACCOUNT\",\"LICENSE\",\"VECHICLE\",\"DEVICE\",\n",
    "              \"BIOID\",\"IDNUM\",\"PHI\"]\n",
    "\n",
    "id_to_label = dict(enumerate(label_name_list))\n",
    "label_to_id = {v: k for k, v in id_to_label.items()}\n",
    "\n",
    "CFG.target_size = len(label_name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TestDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepare_input(cfg, text):\n",
    "    inputs = cfg.tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors=None, \n",
    "        add_special_tokens=True, \n",
    "        max_length=CFG.max_len,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "        \n",
    "    return inputs\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['text'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, self.texts[item])\n",
    "\n",
    "        # token_start = (inputs['attention_mask'] > 0).int()\n",
    "\n",
    "        cls_token_index = 0\n",
    "\n",
    "        sep_token_index = torch.count_nonzero(inputs['attention_mask']).item()-1\n",
    "\n",
    "        token_start = (inputs['attention_mask'] > 0).int()\n",
    "\n",
    "        token_start[cls_token_index] = -1\n",
    "        token_start[sep_token_index] = -1\n",
    "        \n",
    "        return {\n",
    "            'input_ids':inputs['input_ids'],\n",
    "            'attention_mask':inputs['attention_mask'],\n",
    "            'token_start': token_start\n",
    "            }\n",
    "\n",
    "    \n",
    "\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataset = TestDataset(CFG, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(testDataset,\n",
    "                              batch_size = CFG.batch_size,\n",
    "                              shuffle=False,\n",
    "                            #   collate_fn = collate_fn,\n",
    "                              num_workers = CFG.num_workers,\n",
    "                              pin_memory = True,\n",
    "                              drop_last=False)\n",
    "\n",
    "# for step, data in enumerate(test_loader):\n",
    "#     print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(module):\n",
    "    \"\"\"\n",
    "    Freezes module's parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    for parameter in module.parameters():\n",
    "        parameter.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER_Model(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(NER_Model, self).__init__()\n",
    "\n",
    "        self.cfg = CFG\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.config.hidden_dropout_prob = 0\n",
    "        self.config.attention_probs_dropout_prob = 0\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.LayerNorm(self.config.hidden_size),\n",
    "            nn.Linear(self.config.hidden_size, self.cfg.target_size)\n",
    "        )\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids=None, targets=None, input_token_starts = None):\n",
    "        if token_type_ids:\n",
    "            transformer_out = self.model(ids, mask, token_type_ids)\n",
    "        else:\n",
    "            transformer_out = self.model(ids, mask)\n",
    "        \n",
    "        sequence_output = transformer_out[0] # shape : (batch,length,dimension)\n",
    "\n",
    "        # 去除[CLS]标签等位置，获得与label对齐的pre_label表示\n",
    "        # token_sequence_output = [layer[starts.nonzero().squeeze(1)]\n",
    "        #                           for layer, starts in zip(sequence_output, input_token_starts)]\n",
    "        \n",
    "        # 将sequence_output的pred_label维度padding到最大长度\n",
    "        # padded_sequence_output = pad_sequence(token_sequence_output, batch_first=True)\n",
    "        \n",
    "        logits = self.output(sequence_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opensetid_model_path = \"C:/Users/Lab000/Desktop/2023_AI_CUP秋季/code/NER/巡迴課程資料集/FB3_13th_solution/AI_CUP_1/model\"\n",
    "model = NER_Model(CFG.model)\n",
    "\n",
    "state = torch.load(f\"./AI_CUP_3/infer_model/deberta-v3-large_full-data-2_gpt-5/{CFG.model.replace('/', '-')}_best.pth\",\n",
    "                                   map_location=torch.device('cpu'))['model']\n",
    "\n",
    "# state = model.load_state_dict(f\"./AI_CUP_3/infer_model/deberta-v3-large_full-data-2_gpt-5/{CFG.model.replace('/', '-')}_best.pth\")['model']\n",
    "\n",
    "# state = torch.load(f\"{opensetid_model_path}/{CFG.model.replace('/', '-')}_best.pth\",\n",
    "#                                    map_location=torch.device('cpu'))['model']\n",
    "    \n",
    "model.load_state_dict(state)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tags = []\n",
    "\n",
    "for step, data in enumerate(tqdm(test_loader)):\n",
    "    model.eval()\n",
    "    ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "    mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "    token_start = data['token_start'].to(device, dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        logits = model(ids, mask)\n",
    "\n",
    "    token_position = token_start.gt(0)\n",
    "    dims = token_position.shape\n",
    "    \n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    token_position = token_position.detach().cpu().numpy()\n",
    "\n",
    "    for text_no in range(dims[0]):\n",
    "        text_tags = []\n",
    "        for pos in range(dims[1]):\n",
    "            if token_position[text_no,pos]:\n",
    "                \n",
    "                text_tags.append(id_to_label[np.argmax(logits[text_no, pos])])\n",
    "        # print(text_tags)\n",
    "        pred_tags.append(text_tags)\n",
    "\n",
    "    # active_loss = token_position.view(-1) == 1\n",
    "    # active_logits = logits.view(-1, CFG.target_size)[active_loss]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文章總數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"文章總數:\",test_df[\"fid\"].unique().shape[0])\n",
    "print(\"文章總數:\",len(test_df[(test_df[\"idx\"]==0)]) + len(test_df[(test_df[\"idx\"]==1)]) + len(test_df[(test_df[\"idx\"]==2)])) #文章第一句起始為0、1或2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 寫入answer.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,j in enumerate(pred_tags):\n",
    "    pred_tags[index] = [label_to_id [k]for k in pred_tags[index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = [0 if j==33 else j for j in pred_tags[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_continuous_and_non_continuous_numbers_and_indices(input_list):\n",
    "    result = []\n",
    "    current_number = input_list[0]\n",
    "    start_index = 0\n",
    "\n",
    "    for i in range(1, len(input_list)):\n",
    "        if input_list[i] != current_number:\n",
    "            end_index = i - 1\n",
    "            result.append((current_number, (start_index, end_index)))\n",
    "            current_number = input_list[i]\n",
    "            start_index = i\n",
    "\n",
    "    # 處理最後一個連續或非連續數字序列\n",
    "    end_index = len(input_list) - 1\n",
    "    result.append((current_number, (start_index, end_index)))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# post-process-ver-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_label_name_list = [\"IDNUM\",\"MEDICALRECORD\",\"PATIENT\",\"CITY\",\"STATE\",\"ZIP\",\"DEPARTMENT\",\n",
    "                  \"HOSPITAL\",\"DOCTOR\",\"STREET\",\"ORGANIZATION\",\"AGE\",\n",
    "                  \"DATE\",\"TIME\",\"PHONE\"]\n",
    "pre_label_id_list = [label_to_id[label_name] for label_name in pre_label_name_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "lengths = []\n",
    "tk0 = tqdm(test_df['text'].fillna(\"\").values, total=len(test_df))\n",
    "\n",
    "final_pre_text = []\n",
    "\n",
    "\n",
    "for index,text in enumerate(tk0):\n",
    "    \n",
    "    \n",
    "\n",
    "    fid = test_df.loc[index][\"fid\"]\n",
    "    idx = test_df.loc[index][\"idx\"]\n",
    "\n",
    "    # token_list = CFG.tokenizer(text, add_special_tokens=False)['input_ids']\n",
    "    \n",
    "    encoded = CFG.tokenizer(text,\n",
    "                            add_special_tokens=False,\n",
    "                            return_offsets_mapping=True)\n",
    "    \n",
    "    token_list = encoded['input_ids']\n",
    "    offset_mapping_list = encoded['offset_mapping']\n",
    "    \n",
    "\n",
    "    WithoutNULL = [-1 if j==33 else j for j in pred_tags[index]]\n",
    "\n",
    "    pre_text_label_and_index = find_continuous_and_non_continuous_numbers_and_indices(WithoutNULL)\n",
    "    \n",
    "    for position, pre_word_label_and_index in enumerate(pre_text_label_and_index) :\n",
    "        pre_label = pre_word_label_and_index[0]\n",
    "        pre_word_start_end = pre_word_label_and_index[1]\n",
    "\n",
    "        if pre_label != -1 and pre_label in pre_label_id_list:\n",
    "        \n",
    "            start = pre_word_start_end[0]\n",
    "            end = pre_word_start_end[1]\n",
    "            \n",
    "            pre_word_text = CFG.tokenizer.decode(token_list[start:end+1])\n",
    "\n",
    "            if pre_word_text == '':# [507] token decode 為 ''，如果預測為PHI，寫進答案會Submission Error\n",
    "                continue\n",
    "\n",
    "            offset_idx = text.find(pre_word_text)\n",
    "\n",
    "            ########################################################################## \n",
    "            if pre_label == 13: # 針對LOCATION-OTHER，Decode時會少一個空白的處理\n",
    "                white_space_position = []\n",
    "                for i, offset_mapping in enumerate(offset_mapping_list):\n",
    "                    if i == 0:\n",
    "                        continue\n",
    "                    previous_offset_start = offset_mapping_list[i-1][0]\n",
    "                    previous_offset_end = offset_mapping_list[i-1][1]\n",
    "                    current_offset_start = offset_mapping_list[i][0]\n",
    "                    current_offset_end = offset_mapping_list[i][1]\n",
    "                    if previous_offset_end!=current_offset_start:\n",
    "                        white_space_position.append(previous_offset_end)\n",
    "\n",
    "                if len(white_space_position)!=0:\n",
    "                    for position in white_space_position:\n",
    "                        pre_word_text = pre_word_text[:position] + ' ' + pre_word_text[position:]\n",
    "                \n",
    "                offset_idx = text.find(pre_word_text)\n",
    "            ##########################################################################\n",
    "            \n",
    "                \n",
    "            print(f\"{fid}\\t{id_to_label[pre_label]}\\t{idx+offset_idx}\\t{idx+offset_idx+len(pre_word_text)}\\t{pre_word_text}\")\n",
    "\n",
    "            final_pre_text.append(f\"{fid}\\t{id_to_label[pre_label]}\\t{idx+offset_idx}\\t{idx+offset_idx+len(pre_word_text)}\\t{pre_word_text}\")\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./upload_answer/NER/NER_answer.txt','w', encoding='utf-8') as f:\n",
    "        for final_pre in final_pre_text:\n",
    "            f.write(final_pre)\n",
    "            f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_CUP_Github",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
