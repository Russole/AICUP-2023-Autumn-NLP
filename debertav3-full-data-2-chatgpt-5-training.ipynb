{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "\n",
    "VER = 3\n",
    "OUTPUT_DIR = f'./AI_CUP_{VER}'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "# os.system('pip install iterative-stratification==0.1.7')\n",
    "# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset, Features, Value\n",
    "\n",
    "\n",
    "os.system('pip install -q transformers')\n",
    "os.system('pip install -q tokenizers')\n",
    "import tokenizers\n",
    "import transformers\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn import metrics\n",
    "from src.machine_learning_util import set_seed, set_device, init_logger, AverageMeter, to_pickle, unpickle, asMinutes, timeSince"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    EXP_ID = '024'\n",
    "    apex = True\n",
    "    model ='microsoft/deberta-v3-large' # 'microsoft/deberta-large' # 'microsoft/deberta-v3-base' #'microsoft/deberta-v3-large' \n",
    "    seed = 2022 # 42 # 71\n",
    "    n_splits = 4\n",
    "    max_len = 512 # 1429 # 1024 # 512\n",
    "    dropout = 0\n",
    "    target_cols = \"label\"\n",
    "    target_size = None\n",
    "    n_accumulate=1\n",
    "    print_freq = 100\n",
    "    eval_freq = 8500 # 780 * 2 # 390 # 170\n",
    "    min_lr=1e-6\n",
    "    scheduler = 'cosine'\n",
    "    batch_size = 6 # 2 # 4\n",
    "    num_workers = 0 #3\n",
    "    lr = 5e-6 # 3e-6\n",
    "    weigth_decay = 0.01\n",
    "    epochs = 2\n",
    "    n_fold = 4\n",
    "    trn_fold = [i for i in range(n_fold)]\n",
    "    train = True\n",
    "    num_warmup_steps = 0\n",
    "    num_cycles=0.5\n",
    "    debug = False\n",
    "    freezing = True\n",
    "    gradient_checkpoint = True\n",
    "    reinit_layers = 4 # 3\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    max_norm = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+'_train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def find_all(a_string, sub):\n",
    "    result = []\n",
    "    k = 0\n",
    "    while k < len(a_string):\n",
    "        k = a_string.find(sub, k)\n",
    "        if k == -1:\n",
    "            return result\n",
    "        else:\n",
    "            start = k\n",
    "            end = k + len(sub)\n",
    "\n",
    "            result.append(f\"{start} {end}\")\n",
    "            # k += 1 #change to k += len(sub) to not search overlapping results\n",
    "            k += len(sub)\n",
    "\n",
    "    return result\n",
    "    \n",
    "seed_everything(CFG.seed)\n",
    "# seed_everything(seed=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./NER_Dataset\"\n",
    "\n",
    "train_anno_file_path = f\"{data_path}/First_Phase_Text_Dataset_answer/answer.txt\"\n",
    "\n",
    "train_anno_file_path_2 = f\"{data_path}/Second_Phase_Text_Dataset_answer/answer.txt\"\n",
    "\n",
    "val_anno_file_path = f\"{data_path}/Validation_Release_answer/answer.txt\"\n",
    "train_report_file_path = f\"{data_path}/First_Phase_Text_Dataset/\"\n",
    "\n",
    "train_report_file_path_2 = f\"{data_path}/Second_Phase_Text_Dataset/\"\n",
    "\n",
    "val_report_file_path = f\"{data_path}/Validation_Release\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_file(anno_file_path):\n",
    "    file = open(anno_file_path, 'r', encoding=\"UTF-8-sig\")\n",
    "    file_Lines = file.readlines()\n",
    "    return file_Lines\n",
    "\n",
    "def process_annotation_file(lines):\n",
    "    entity_dict = {}\n",
    "    for line in lines:\n",
    "        items = line.strip('\\n').split('\\t')\n",
    "        if len(items) == 5:\n",
    "            item_dict = {'phi' : items[1],\n",
    "                         'st_idx' : int(items[2]),\n",
    "                         'ed_idx' : int(items[3]),\n",
    "                         'entity' : items[4]}\n",
    "        elif len(items) == 6:\n",
    "            item_dict = {'phi' : items[1],\n",
    "                         'st_idx' : int(items[2]),\n",
    "                         'ed_idx' : int(items[3]),\n",
    "                         'entity' : items[4],\n",
    "                         'normalize_time' : items[5]}\n",
    "        if items[0] not in entity_dict:\n",
    "            entity_dict[items[0]] = [item_dict]\n",
    "        else :\n",
    "            entity_dict[items[0]].append(item_dict)\n",
    "    return  entity_dict\n",
    "\n",
    "def process_medical_report(txt_name, report_file_path, annos_dict,special_tokens_dict): # annos_dict : 標註答案\n",
    "    \n",
    "    file_name = txt_name+'.txt'\n",
    "    sents = read_file(os.path.join(report_file_path, file_name))\n",
    "\n",
    "    article = \"\".join(sents) # 病人報告\n",
    "\n",
    "\n",
    "    bounary, item_idx, temp_seq, seq_pairs = 0, 0, \"\", []\n",
    "\n",
    "    for w_idx, word in enumerate(article):\n",
    "        if w_idx == annos_dict[txt_name][item_idx]['st_idx']: # w_idx == start\n",
    "            phi_key = annos_dict[txt_name][item_idx]['phi'] # phi類別\n",
    "            phi_value = annos_dict[txt_name][item_idx]['entity'] # entity\n",
    "\n",
    "            if 'normalize_time' in annos_dict[txt_name][item_idx]:\n",
    "                temp_seq += f\"{phi_key}:{phi_value}=>{annos_dict[txt_name][item_idx]['normalize_time']}\\n\"\n",
    "            else:\n",
    "                temp_seq += f\"{phi_key}:{phi_value}\\n\"\n",
    "            if item_idx == len(annos_dict[txt_name]) - 1: # 還有答案就繼續\n",
    "                continue\n",
    "            item_idx += 1 # 下一個正確答案\n",
    "        if word == '\\n':\n",
    "            new_line_idx = w_idx + 1\n",
    "            if temp_seq == \"\": # 都沒有正確答案\n",
    "                temp_seq = \"PHI:NULL\"\n",
    "            # seq_pair = special_tokens_dict['bos_token'] + article[bounary:new_line_idx] + special_tokens_dict['sep_token'] + temp_seq + special_tokens_dict['eos_token']\n",
    "            seq_pair = article[bounary:new_line_idx] +'[SEP]' +temp_seq\n",
    "            bounary = new_line_idx\n",
    "            seq_pairs.append(seq_pair)\n",
    "            temp_seq = \"\" # 初始化\n",
    "    return seq_pairs\n",
    "\n",
    "def generate_annotated_medical_report(anno_file_path, report_file_path):\n",
    "    anno_lines = read_file(anno_file_path)\n",
    "    annos_dict = process_annotation_file(anno_lines)\n",
    "    report_name_list = [f.split('.')[0] for f in os.listdir(report_file_path) if os.path.isfile(os.path.join(report_file_path, f))]\n",
    "    \n",
    "    special_tokens_dict = {\"bos_token\":\"<|endoftext|>\",\"sep_token\":\"\\n####\\n\\n\",\"eos_token\":\"<|END|>\"}\n",
    "    \n",
    "    pocessed_files={}\n",
    "\n",
    "    # test = []\n",
    "    for txt_name in report_name_list:\n",
    "        pocessed_file = process_medical_report(txt_name, report_file_path,annos_dict,special_tokens_dict)\n",
    "        pocessed_files[txt_name] = pocessed_file\n",
    "\n",
    "        #test.append(pocessed_file)\n",
    "    \n",
    "    return pocessed_files #test\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_report = generate_annotated_medical_report(train_anno_file_path,train_report_file_path)\n",
    "\n",
    "train_report_2 = generate_annotated_medical_report(train_anno_file_path_2,train_report_file_path_2)\n",
    "\n",
    "val_report = generate_annotated_medical_report(val_anno_file_path,val_report_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = []\n",
    "val_list = []\n",
    "for key,value in train_report.items():\n",
    "    train_list.extend(value)\n",
    "\n",
    "for key,value in train_report_2.items():\n",
    "    train_list.extend(value)\n",
    "\n",
    "for key,value in val_report.items():\n",
    "    train_list.extend(value)\n",
    "\n",
    "for key,value in val_report.items():\n",
    "    val_list.extend(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_list,columns=[\"content_label\"])\n",
    "val_df = pd.DataFrame(val_list,columns=[\"content_label\"])\n",
    "train_df = train_df.drop(train_df[train_df[\"content_label\"] == \"\\n[SEP]PHI:NULL\"].index)\n",
    "val_df = val_df.drop(val_df[val_df[\"content_label\"] == \"\\n[SEP]PHI:NULL\"].index)\n",
    "train_df = train_df.reset_index(drop = True)\n",
    "val_df = val_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(df):\n",
    "    text, label = df[\"content_label\"].split(\"\\n[SEP]\")\n",
    "    return text\n",
    "\n",
    "def split_label(df):\n",
    "    text, label = df[\"content_label\"].split(\"\\n[SEP]\")\n",
    "    return label\n",
    "\n",
    "\n",
    "train_df[\"text\"] = train_df.apply(split_text,axis = 1)\n",
    "train_df[\"label\"] = train_df.apply(split_label,axis = 1)\n",
    "val_df[\"text\"] = val_df.apply(split_text,axis = 1)\n",
    "val_df[\"label\"] = val_df.apply(split_label,axis = 1)\n",
    "\n",
    "train_df = train_df.drop(columns=['content_label'])\n",
    "\n",
    "val_df = val_df.drop(columns=['content_label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Chatgpt generated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_label_text_list = []\n",
    "gpt_text_list = []\n",
    "\n",
    "gpt_data_path = \"./NER_Dataset/CHATGPT/VER-5\"\n",
    "\n",
    "country_label_text_list = read_file(f'{gpt_data_path}/COUNTRY_VER-5/country.txt')\n",
    "organization_text_list = read_file(f'{gpt_data_path}/ORGANIZATION_VER-5/Corporation_INC_company/ORGANIZATION_text.txt')\n",
    "organization_label_text_list = read_file(f'{gpt_data_path}/ORGANIZATION_VER-5/Corporation_INC_company/ORGANIZATION_label.txt')\n",
    "\n",
    "organization_label_text_list_2 = read_file(f'{gpt_data_path}/ORGANIZATION_VER-5/ORGANIZATION_label_text.txt')\n",
    "\n",
    "LOCATION_OTHER_label_text_list_1 = read_file(f'{gpt_data_path}/LOCATION-OTHER_VER-5/Correct-Format-LOCATION-OTHER_FORMAT-1.txt')\n",
    "# LOCATION_OTHER_label_text_list_2 = read_file('C:/Users/Lab000/Desktop/2023_AI_CUP秋季/data/CHATGPT/VER-3/LOCATION-OTHER_VER-3/LOCATION-OTHER_FORMAT-2.txt')\n",
    "\n",
    "PHONE_label_text_list_1 = read_file(f'{gpt_data_path}/PHONE_VER-5/PHONE_text-label_FORMAT-1.txt')\n",
    "PHONE_label_text_list_2 = read_file(f'{gpt_data_path}/PHONE_VER-5/PHONE_text-label_FORMAT-2.txt')\n",
    "\n",
    "\n",
    "for country_label_text in country_label_text_list:\n",
    "    label_text = country_label_text.split(': ')[0]\n",
    "    text = country_label_text.split(': ')[1].strip('\\n')\n",
    "    label_text = \"COUNTRY:\"+label_text+\"\\n\"\n",
    "    gpt_label_text_list.append(label_text)\n",
    "    gpt_text_list.append(text)\n",
    "\n",
    "for pos, organization_text in enumerate(organization_text_list):\n",
    "    text = organization_text.strip('\\n')\n",
    "    label_text = 'ORGANIZATION:'+organization_label_text_list[pos]\n",
    "    gpt_label_text_list.append(label_text)\n",
    "    gpt_text_list.append(text)\n",
    "\n",
    "for organization_text in organization_label_text_list_2:\n",
    "    label_text = organization_text.split(': ')[0]\n",
    "    text = organization_text.split(': ')[1].strip('\\n')\n",
    "    label_text = \"ORGANIZATION:\"+label_text+\"\\n\"\n",
    "    gpt_label_text_list.append(label_text)\n",
    "    gpt_text_list.append(text)\n",
    "\n",
    "\n",
    "for pos, LOCATION_OTHER_text in enumerate(LOCATION_OTHER_label_text_list_1):\n",
    "    text = LOCATION_OTHER_text.strip('\\n')\n",
    "    label_text = 'LOCATION-OTHER:'+text+\"\\n\"\n",
    "    gpt_label_text_list.append(label_text)\n",
    "    gpt_text_list.append(text)\n",
    "\n",
    "for PHONE_label_text in PHONE_label_text_list_1:\n",
    "    label_text = PHONE_label_text.split(': ')[0]\n",
    "    text = PHONE_label_text.split(': ')[1].strip('\\n')\n",
    "    label_text = \"PHONE:\"+label_text+\"\\n\"\n",
    "    gpt_label_text_list.append(label_text)\n",
    "    gpt_text_list.append(text)\n",
    "\n",
    "for PHONE_label_text in PHONE_label_text_list_2:\n",
    "    label_text = PHONE_label_text.split(': ')[0]\n",
    "    text = PHONE_label_text.split(': ')[1].strip('\\n')\n",
    "    label_text = \"PHONE:\"+label_text+\"\\n\"\n",
    "    gpt_label_text_list.append(label_text)\n",
    "    gpt_text_list.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_train_df= pd.DataFrame(data={'text':gpt_text_list,'label':gpt_label_text_list})\n",
    "GPT_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(GPT_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, GPT_train_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name_list = [\"PATIENT\",\"DOCTOR\",\"USERNAME\",\"PROFESSION\",\"ROOM\",\"DEPARTMENT\",\"HOSPITAL\",\n",
    "              \"ORGANIZATION\",\"STREET\",\"CITY\",\"STATE\",\"COUNTRY\",\"ZIP\",\"LOCATION-OTHER\",\n",
    "              \"AGE\",\"DATE\",\"TIME\",\"DURATION\",\"SET\",\"PHONE\",\"FAX\",\"EMAIL\",\"URL\",\"IPADDR\",\n",
    "              \"SSN\",\"MEDICALRECORD\",\"HEALTHPLAN\",\"ACCOUNT\",\"LICENSE\",\"VECHICLE\",\"DEVICE\",\n",
    "              \"BIOID\",\"IDNUM\",\"PHI\"]\n",
    "\n",
    "id_to_label = dict(enumerate(label_name_list))\n",
    "label_to_id = {v: k for k, v in id_to_label.items()}\n",
    "\n",
    "CFG.target_size = len(label_name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# content max len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Define max_len\n",
    "# ====================================================\n",
    "lengths = []\n",
    "tk0 = tqdm(train_df['text'].fillna(\"\").values, total=len(train_df))\n",
    "for text in tk0:\n",
    "    length = len(CFG.tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    lengths.append(length)\n",
    "print(max(lengths) + 2)\n",
    "CFG.max_len = max(lengths) + 2 # cls & sep\n",
    "\n",
    "# LOGGER.info(f\"input column max_len: {CFG.max_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 找尋子字串的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = find_all(\"SWAN HILL DISTRICT HEALTH [SWAN HILL\",\"SWAN\")\n",
    "result = \";\".join(result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label & Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = []\n",
    "all_annotations = []\n",
    "all_label_list = []\n",
    "\n",
    "all_label_text = []\n",
    "all_annotation_length = []\n",
    "for ind in train_df.index:\n",
    "    annotation_length = 0 # annotation個數\n",
    "    sub_text_list = []\n",
    "    annotation = []\n",
    "    label_list = []\n",
    "    text = train_df['text'][ind] \n",
    "    label_text = train_df['label'][ind]\n",
    "    if \"\\n\" in label_text:\n",
    "        label_text_list = label_text.split(\"\\n\") # 分開多個label\n",
    "        label_text_list.remove(\"\")\n",
    "\n",
    "        for label_text in label_text_list:\n",
    "            if(label_text.split(\":\")[0] == \"TIME\"):\n",
    "                label_id_name = \"TIME\"\n",
    "                temp_text_label, _ = label_text.split(\"=>\")\n",
    "\n",
    "                _ , sub_text=temp_text_label.split(\"TIME:\")\n",
    "\n",
    "            elif(label_text.split(\":\")[0] == \"DATE\"):\n",
    "            # if len(label_text.split(\":\")) !=2:\n",
    "                label_id_name = \"DATE\"\n",
    "\n",
    "                temp_text_label, _ = label_text.split(\"=>\")\n",
    "\n",
    "                \n",
    "                _ , sub_text=temp_text_label.split(\"DATE:\")\n",
    "            elif(label_text.split(\":\")[0] == \"URL\"):\n",
    "\n",
    "                label_id_name = \"URL\"\n",
    "\n",
    "                _ , sub_text=label_text.split(\"URL:\")\n",
    "            elif(label_text.split(\":\")[0] == \"DURATION\"):\n",
    "                \n",
    "                label_id_name = \"DURATION\"\n",
    "\n",
    "                temp_text_label, _ = label_text.split(\"=>\")\n",
    "\n",
    "                \n",
    "                _ , sub_text=temp_text_label.split(\"DURATION:\")\n",
    "\n",
    "            elif(label_text.split(\":\")[0] == \"SET\"):\n",
    "                \n",
    "                label_id_name = \"SET\"\n",
    "\n",
    "                temp_text_label, _ = label_text.split(\"=>\")\n",
    "\n",
    "                \n",
    "                _ , sub_text=temp_text_label.split(\"SET:\")            \n",
    "                \n",
    "            else :\n",
    "                label_id_name, sub_text = label_text.split(\":\") # label子字串是哪種PHI, text中的label子字串\n",
    "\n",
    "            label_id = label_to_id[label_id_name]\n",
    "\n",
    "            result = find_all(text, sub_text)\n",
    "\n",
    "            if(len(result)==0):\n",
    "                print(f\"Index:{ind},PHI:{label_id_name}, text:{text},sub_text:{sub_text}, do not match\")\n",
    "\n",
    "            else:\n",
    "\n",
    "                annotation_length += len(result) # 計算annotation個數\n",
    "\n",
    "                for sub_result in result:\n",
    "                    sub_text_list.append(sub_text)\n",
    "                    annotation.append(sub_result)\n",
    "                    label_list.append(label_id)\n",
    "\n",
    "            \n",
    "    else :\n",
    "        # print(\"PHI NULL\")\n",
    "        sub_text_list.append(\"NULL\")\n",
    "        annotation.append(\"NULL\")\n",
    "        label_list.append(label_to_id[\"PHI\"]) # if sub_text not match, label connect PHI\n",
    "\n",
    "    all_text.append(text)\n",
    "    all_label_text.append(sub_text_list)\n",
    "    all_annotations.append(annotation)\n",
    "    all_annotation_length.append(annotation_length)\n",
    "    all_label_list.append(label_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_val_text = []\n",
    "all_val_annotations = []\n",
    "all_val_label_list = []\n",
    "all_val_label_text = []\n",
    "all_val_annotation_length = []\n",
    "for ind in val_df.index:\n",
    "    annotation_length = 0 # annotation個數\n",
    "    sub_text_list = []\n",
    "    annotation = []\n",
    "    label_list = []\n",
    "    text = val_df['text'][ind] \n",
    "    label_text = val_df['label'][ind]\n",
    "    if \"\\n\" in label_text:\n",
    "        label_text_list = label_text.split(\"\\n\") # 分開多個label\n",
    "        label_text_list.remove(\"\")\n",
    "\n",
    "        for label_text in label_text_list:\n",
    "            if(label_text.split(\":\")[0] == \"TIME\"):\n",
    "                label_id_name = \"TIME\"\n",
    "                temp_text_label, _ = label_text.split(\"=>\")\n",
    "\n",
    "                _ , sub_text=temp_text_label.split(\"TIME:\")\n",
    "\n",
    "            elif(label_text.split(\":\")[0] == \"DATE\"):\n",
    "            # if len(label_text.split(\":\")) !=2:\n",
    "                label_id_name = \"DATE\"\n",
    "\n",
    "                temp_text_label, _ = label_text.split(\"=>\")\n",
    "\n",
    "                \n",
    "                _ , sub_text=temp_text_label.split(\"DATE:\")\n",
    "            elif(label_text.split(\":\")[0] == \"URL\"):\n",
    "\n",
    "                label_id_name = \"URL\"\n",
    "\n",
    "                _ , sub_text=label_text.split(\"URL:\")\n",
    "            elif(label_text.split(\":\")[0] == \"DURATION\"):\n",
    "                \n",
    "                label_id_name = \"DURATION\"\n",
    "\n",
    "                temp_text_label, _ = label_text.split(\"=>\")\n",
    "\n",
    "                \n",
    "                _ , sub_text=temp_text_label.split(\"DURATION:\")\n",
    "\n",
    "            elif(label_text.split(\":\")[0] == \"SET\"):\n",
    "                \n",
    "                label_id_name = \"SET\"\n",
    "\n",
    "                temp_text_label, _ = label_text.split(\"=>\")\n",
    "\n",
    "                \n",
    "                _ , sub_text=temp_text_label.split(\"SET:\")            \n",
    "                \n",
    "            else :\n",
    "                label_id_name, sub_text = label_text.split(\":\") # label子字串是哪種PHI, text中的label子字串\n",
    "\n",
    "            label_id = label_to_id[label_id_name]\n",
    "\n",
    "            result = find_all(text, sub_text)\n",
    "\n",
    "            if(len(result)==0):\n",
    "                print(f\"Index:{ind},PHI:{label_id_name}, text:{text},sub_text:{sub_text}, do not match\")\n",
    "\n",
    "            else:\n",
    "\n",
    "                annotation_length += len(result) # 計算annotation個數\n",
    "\n",
    "                for sub_result in result:\n",
    "                    sub_text_list.append(sub_text)\n",
    "                    annotation.append(sub_result)\n",
    "                    label_list.append(label_id)\n",
    "            \n",
    "    else :\n",
    "        # print(\"PHI NULL\")\n",
    "        sub_text_list.append(\"NULL\")\n",
    "        annotation.append(\"NULL\")\n",
    "        label_list.append(label_to_id[\"PHI\"]) # if sub_text not match, label connect PHI\n",
    "\n",
    "    all_val_text.append(text)\n",
    "    all_val_label_text.append(sub_text_list)\n",
    "    all_val_annotations.append(annotation)\n",
    "    all_val_annotation_length.append(annotation_length)\n",
    "    all_val_label_list.append(label_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 創建Train Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_df = pd.DataFrame(list(zip(all_text, all_label_text,all_label_list, all_annotations, all_annotation_length)), \n",
    "columns =['text', 'label_text','label', 'annotation', 'annotation_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"原本train dataframe筆數:\",len(train_df))\n",
    "print(\"新train dataframe筆數:\",len(new_train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 計算訓練集中各PHI數量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_label_count = {k : 0 for k, v in id_to_label.items()}\n",
    "label_id = label_to_id[\"FAX\"] # DATE、DEPARTMENT少2筆，DOCTOR少4筆，ORGANIZATION少1筆\n",
    "iteration = 0\n",
    "count = 0\n",
    "for ind in new_train_df.index:\n",
    "    # if label_id in new_train_df.loc[ind].label:\n",
    "        \n",
    "    label_id_list = new_train_df.loc[ind].label\n",
    "    label_id_count_dict = {i:label_id_list.count(i) for i in label_id_list}\n",
    "\n",
    "    for label_id in label_id_list:\n",
    "        all_label_count[label_id]+=label_id_count_dict[label_id]\n",
    "\n",
    "all_label_count = {id_to_label[k] : v for k, v in all_label_count.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 創建Val Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_val_df = pd.DataFrame(list(zip(all_val_text, all_val_label_text,all_val_label_list, all_val_annotations, all_val_annotation_length)), \n",
    "columns =['text', 'label_text','label', 'annotation', 'annotation_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"原本val dataframe筆數:\",len(val_df))\n",
    "print(\"新val dataframe筆數:\",len(new_val_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepare_input(cfg, text):\n",
    "    #print(text)\n",
    "    inputs = cfg.tokenizer(text, \n",
    "                           add_special_tokens=True,\n",
    "                           max_length=CFG.max_len,\n",
    "                           padding=\"max_length\",\n",
    "                           return_offsets_mapping=False)\n",
    "    \n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def create_label(cfg, text, label_list, location_list, annotation_length):\n",
    "\n",
    "    encoded = cfg.tokenizer(text,\n",
    "                            add_special_tokens=True,\n",
    "                            max_length=CFG.max_len,\n",
    "                            padding=\"max_length\",\n",
    "                            return_offsets_mapping=True)\n",
    "    offset_mapping = encoded['offset_mapping']\n",
    "\n",
    "\n",
    "    ignore_idxes = np.where(np.array(encoded.sequence_ids()) != 0)[0] # ignore special & padding token\n",
    "\n",
    "\n",
    "\n",
    "    label = np.zeros(len(offset_mapping))# EXAMPLE 1 : [32], EXAMPLE 2 : [1, 1, 1, 1]\n",
    "\n",
    "    # 將矩陣中的0替換為PHI:NULL 標籤的數字\n",
    "\n",
    "    PHI_NULL_ID  = label_to_id[\"PHI\"] \n",
    "    label[label == 0] = PHI_NULL_ID\n",
    "\n",
    "    label[ignore_idxes] = -1\n",
    "\n",
    "    if annotation_length != 0:\n",
    "    # if location_list[0] != \"NULL\":\n",
    "        for step, location in enumerate(location_list):# EXAMPLE 1 : [13 23], EXAMPLE 2 : [37 46, 57 63, 76 84, 89 97], EXAMPLE 3 : [9 17, 18 26, 9 17, 18 26]\n",
    "            \n",
    "            # for loc in [s.split() for s in location.split(' ')]:\n",
    "            \n",
    "                start_idx = -1\n",
    "                end_idx = -1\n",
    "\n",
    "    \n",
    "\n",
    "                # start, end = int(loc[0]), int(loc[1]) # example start : 696, end : 724\n",
    "                start, end = location.split(' ')\n",
    "                start = int(start)\n",
    "                end = int(end)\n",
    "                for idx in range(len(offset_mapping)): # 走訪offset_mapping\n",
    "                    if (start_idx == -1) & (start < offset_mapping[idx][0]):\n",
    "                        start_idx = idx - 1 # example 180 = 181 - 1\n",
    "\n",
    "                        \n",
    "\n",
    "                    if (end_idx == -1) & (end <= offset_mapping[idx][1]):\n",
    "                        end_idx = idx + 1 # example 187 = 186 + 1\n",
    "\n",
    "                        \n",
    "\n",
    "                if start_idx == -1:\n",
    "                    start_idx = end_idx\n",
    "                if (start_idx != -1) & (end_idx != -1):\n",
    "                    label_id = label_list[step]\n",
    "                    label[start_idx:end_idx] = label_id\n",
    "\n",
    "        \n",
    "    return torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.text = df['text'].values\n",
    "        self.label_text = df['label_text'].values\n",
    "        self.label = df['label'].values\n",
    "        self.locations = df['annotation'].values\n",
    "\n",
    "        self.annotation_lengths = df['annotation_length'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, \n",
    "                               self.text[item])\n",
    "        \n",
    "        label = create_label(self.cfg, \n",
    "                             self.text[item], \n",
    "                             self.label[item], \n",
    "                             self.locations[item],\n",
    "                             self.annotation_lengths[item])\n",
    "        \n",
    "        return {\n",
    "            'input_ids':inputs['input_ids'],\n",
    "            'attention_mask':inputs['attention_mask'],\n",
    "            'label':label,\n",
    "            }\n",
    "        # return inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(module):\n",
    "    \"\"\"\n",
    "    Freezes module's parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    for parameter in module.parameters():\n",
    "        parameter.requires_grad = False\n",
    "\n",
    "def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "    if cfg.scheduler == 'linear':\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "        )\n",
    "    elif cfg.scheduler == 'cosine':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "        )\n",
    "    return scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER_Model(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(NER_Model, self).__init__()\n",
    "\n",
    "        self.cfg = CFG\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.config.hidden_dropout_prob = 0\n",
    "        self.config.attention_probs_dropout_prob = 0\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.LayerNorm(self.config.hidden_size),\n",
    "            nn.Linear(self.config.hidden_size, self.cfg.target_size)\n",
    "        )\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids=None, targets=None, input_token_starts = None):\n",
    "        if token_type_ids:\n",
    "            transformer_out = self.model(ids, mask, token_type_ids)\n",
    "        else:\n",
    "            transformer_out = self.model(ids, mask)\n",
    "        \n",
    "        sequence_output = transformer_out[0] # shape : (batch,length,dimension)\n",
    "\n",
    "        # 去除[CLS]标签等位置，获得与label对齐的pre_label表示\n",
    "        # token_sequence_output = [layer[starts.nonzero().squeeze(1)]\n",
    "        #                           for layer, starts in zip(sequence_output, input_token_starts)]\n",
    "        \n",
    "        # 将sequence_output的pred_label维度padding到最大长度\n",
    "        # padded_sequence_output = pad_sequence(token_sequence_output, batch_first=True)\n",
    "        \n",
    "        logits = self.output(sequence_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(logits, labels):\n",
    "\n",
    "    if labels is not None:\n",
    "        \n",
    "        loss_mask = labels.gt(-1) # 大於 -1 => True\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        # Only keep active parts of the loss\n",
    "        if loss_mask is not None:\n",
    "        # 只留下label存在的位置计算loss\n",
    "            active_loss = loss_mask.view(-1) == 1\n",
    "            active_logits = logits.view(-1, CFG.target_size)[active_loss]\n",
    "            active_labels = labels.view(-1)[active_loss]\n",
    "            loss = loss_fct(active_logits, active_labels)\n",
    "        else:\n",
    "            loss = loss_fct(logits.view(-1, CFG.target_size), labels.view(-1))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch, best_score, best_valid_loss, valid_loader):\n",
    "    model.train()\n",
    "    scaler = GradScaler(enabled=CFG.apex)\n",
    "\n",
    "    dataset_size = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    start = end = time.time()\n",
    "\n",
    "    for step, data in enumerate(dataloader):\n",
    "\n",
    "        \n",
    "        \n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        labels = data['label'].to(device, dtype=torch.long)\n",
    "\n",
    "        # label_start = data['label_start'].to(device, dtype=torch.float)\n",
    "\n",
    "        batch_size = ids.size(0)\n",
    "\n",
    "        with autocast(enabled=CFG.apex):\n",
    "            logits = model(ids, mask)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "        #accumulate\n",
    "        loss = loss / CFG.n_accumulate\n",
    "        scaler.scale(loss).backward()\n",
    "        if (step +1) % CFG.n_accumulate == 0:#n_accumulate=1\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        if step % CFG.print_freq == 0 or step == (len(dataloader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Loss: [{3}]'\n",
    "                  'Elapsed {remain:s} '\n",
    "                  .format(epoch+1, step, len(dataloader), epoch_loss,\n",
    "                          remain=timeSince(start, float(step+1)/len(dataloader))))\n",
    "\n",
    "        if (step > 0) & (step % CFG.eval_freq == 0) :\n",
    "\n",
    "            valid_epoch_loss = valid_one_epoch(model, valid_loader, device, epoch)\n",
    "\n",
    "            # score = get_score(pred, valid_labels)\n",
    "\n",
    "            LOGGER.info(f'Epoch {epoch+1} Step {step} - avg_train_loss: {epoch_loss:.4f}  avg_val_loss: {valid_epoch_loss:.4f}')\n",
    "            # LOGGER.info(f'Epoch {epoch+1} Step {step} - Score: {score:.4f}')\n",
    "\n",
    "            if valid_epoch_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_epoch_loss\n",
    "                LOGGER.info(f'Epoch {epoch+1} Step {step} - Save Best Loss: {best_valid_loss:.4f} Model')\n",
    "                torch.save({'model': model.state_dict()},\n",
    "                            # 'predictions': pred},\n",
    "                            OUTPUT_DIR+f\"/model/{CFG.model.replace('/', '-')}_best.pth\")\n",
    "\n",
    "            # model.train()\n",
    "\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return epoch_loss, best_valid_loss # valid_epoch_loss, pred, best_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    dataset_size = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    start = end = time.time()\n",
    "    pred = []\n",
    "\n",
    "    for step, data in enumerate(dataloader):\n",
    "        ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "        labels = data['label'].to(device, dtype=torch.long)\n",
    "\n",
    "        # label_start = data['label_start'].to(device, dtype=torch.float)\n",
    "\n",
    "        batch_size = ids.size(0)\n",
    "        outputs = model(ids, mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # pred.append(outputs.to('cpu').numpy())\n",
    "\n",
    "        running_loss += (loss.item()* batch_size)\n",
    "        dataset_size += batch_size\n",
    "\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "\n",
    "        end = time.time()\n",
    "\n",
    "        if step % CFG.print_freq == 0 or step == (len(dataloader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  .format(step, len(dataloader),\n",
    "                          remain=timeSince(start, float(step+1)/len(dataloader))))\n",
    "\n",
    "    # pred = np.concatenate(pred)\n",
    "    return epoch_loss #, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(fold):\n",
    "    # LOGGER.info(f'-------------fold:{fold} training-------------')\n",
    "\n",
    "    # train_data = train[train.fold != fold].reset_index(drop=True)\n",
    "    # valid_data = train[train.fold == fold].reset_index(drop=True)\n",
    "    # valid_labels = valid_data[CFG.targets].values\n",
    "\n",
    "    trainDataset = TrainDataset(CFG, new_train_df)\n",
    "    validDataset = TrainDataset(CFG, new_val_df)\n",
    "    \n",
    "    \n",
    "\n",
    "    train_loader = DataLoader(trainDataset,\n",
    "                              batch_size = CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                            #   collate_fn = collate_fn,\n",
    "                              num_workers = CFG.num_workers,\n",
    "                              pin_memory = True,\n",
    "                              drop_last=True)\n",
    "\n",
    "    \n",
    "    valid_loader = DataLoader(validDataset,\n",
    "                              # batch_size = CFG.batch_size * 2,\n",
    "                              batch_size = CFG.batch_size,\n",
    "                              shuffle=False,\n",
    "                              # collate_fn = collate_fn,\n",
    "                              num_workers = CFG.num_workers,\n",
    "                              pin_memory = True,\n",
    "                              drop_last=False)\n",
    "\n",
    "    model = NER_Model(CFG.model)\n",
    "    torch.save(model.config, OUTPUT_DIR+'/model/config.pth')\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weigth_decay)\n",
    "    num_train_steps = int(len(train_df) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # loop\n",
    "    best_score = 100\n",
    "\n",
    "    best_valid_loss = 100 # 之前epoch最低的loss\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_epoch_loss, valid_epoch_loss = train_one_epoch(model, optimizer, scheduler, train_loader, device, epoch, best_score, best_valid_loss, valid_loader)\n",
    "        # valid_epoch_loss : 當前epoch最低的loss\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        # LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {train_epoch_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {train_epoch_loss:.4f}  avg_val_loss: {valid_epoch_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        # LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n",
    "\n",
    "        if valid_epoch_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_epoch_loss\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict()},\n",
    "                        OUTPUT_DIR+f\"/model/{CFG.model.replace('/', '-')}_best.pth\")\n",
    "\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_CUP_Github",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
