{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers.__version__: 0.13.3\n",
      "transformers.__version__: 4.29.2\n",
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "# os.system('pip install iterative-stratification==0.1.7')\n",
    "# from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset, Features, Value\n",
    "\n",
    "\n",
    "os.system('pip install -q transformers')\n",
    "os.system('pip install -q tokenizers')\n",
    "import tokenizers\n",
    "import transformers\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn import metrics\n",
    "# from src.machine_learning_util import set_seed, set_device, init_logger, AverageMeter, to_pickle, unpickle, asMinutes, timeSince"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "class CFG:\n",
    "    EXP_ID = '024'\n",
    "    apex = True\n",
    "    model ='microsoft/deberta-v3-large' # 'microsoft/deberta-large' # 'microsoft/deberta-v3-base' #'microsoft/deberta-v3-large' \n",
    "    seed = 2022 # 42 # 71\n",
    "    n_splits = 4\n",
    "    max_len = 640 + 2 # 1429 # 1024 # 512\n",
    "    dropout = 0\n",
    "    target_cols = \"label\"\n",
    "    target_size = None\n",
    "    n_accumulate=1\n",
    "    print_freq = 100\n",
    "    eval_freq = 780 * 2 # 390 # 170\n",
    "    min_lr=1e-6\n",
    "    scheduler = 'cosine'\n",
    "    batch_size = 12 # 2 # 4\n",
    "    num_workers = 0 #3\n",
    "    lr = 5e-6 # 3e-6\n",
    "    weigth_decay = 0.01\n",
    "    epochs = 3\n",
    "    n_fold = 4\n",
    "    trn_fold = [i for i in range(n_fold)]\n",
    "    train = True\n",
    "    num_warmup_steps = 0\n",
    "    num_cycles=0.5\n",
    "    debug = False\n",
    "    freezing = True\n",
    "    gradient_checkpoint = True\n",
    "    reinit_layers = 4 # 3\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    max_norm = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "           \n",
    "seed_everything(CFG.seed)\n",
    "# seed_everything(seed=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_report_file_path = \"./NER_Dataset/private_data\" #巡迴課程 val 資料集\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=f\"{test_report_file_path}/demo.tsv\", delimiter='\\t',\n",
    "                       features = Features({\n",
    "                              'fid': Value('string'), 'idx': Value('int64'),\n",
    "                              'text': Value('string')}),\n",
    "                              column_names=['fid', 'idx', 'text'], keep_default_na=False)\n",
    "\n",
    "test_list = list(dataset[\"train\"])\n",
    "\n",
    "test_df = pd.DataFrame.from_dict(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fid</th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1097</td>\n",
       "      <td>1</td>\n",
       "      <td>433475.RDC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1097</td>\n",
       "      <td>12</td>\n",
       "      <td>Timmins, ELDEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1097</td>\n",
       "      <td>27</td>\n",
       "      <td>43J47561,43J47561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1097</td>\n",
       "      <td>46</td>\n",
       "      <td>Last edited : 7/9/2063  Page: 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1097</td>\n",
       "      <td>78</td>\n",
       "      <td>CLINICAL:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fid  idx                             text\n",
       "0  1097    1                       433475.RDC\n",
       "1  1097   12                   Timmins, ELDEN\n",
       "2  1097   27                43J47561,43J47561\n",
       "3  1097   46  Last edited : 7/9/2063  Page: 2\n",
       "4  1097   78                        CLINICAL:"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name_list = [\"PATIENT\",\"DOCTOR\",\"USERNAME\",\"PROFESSION\",\"ROOM\",\"DEPARTMENT\",\"HOSPITAL\",\n",
    "              \"ORGANIZATION\",\"STREET\",\"CITY\",\"STATE\",\"COUNTRY\",\"ZIP\",\"LOCATION-OTHER\",\n",
    "              \"AGE\",\"DATE\",\"TIME\",\"DURATION\",\"SET\",\"PHONE\",\"FAX\",\"EMAIL\",\"URL\",\"IPADDR\",\n",
    "              \"SSN\",\"MEDICALRECORD\",\"HEALTHPLAN\",\"ACCOUNT\",\"LICENSE\",\"VECHICLE\",\"DEVICE\",\n",
    "              \"BIOID\",\"IDNUM\",\"PHI\"]\n",
    "\n",
    "id_to_label = dict(enumerate(label_name_list))\n",
    "label_to_id = {v: k for k, v in id_to_label.items()}\n",
    "\n",
    "CFG.target_size = len(label_name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TestDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepare_input(cfg, text):\n",
    "    inputs = cfg.tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors=None, \n",
    "        add_special_tokens=True, \n",
    "        max_length=CFG.max_len,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "        \n",
    "    return inputs\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['text'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, self.texts[item])\n",
    "\n",
    "        # token_start = (inputs['attention_mask'] > 0).int()\n",
    "\n",
    "        cls_token_index = 0\n",
    "\n",
    "        sep_token_index = torch.count_nonzero(inputs['attention_mask']).item()-1\n",
    "\n",
    "        token_start = (inputs['attention_mask'] > 0).int()\n",
    "\n",
    "        token_start[cls_token_index] = -1\n",
    "        token_start[sep_token_index] = -1\n",
    "        \n",
    "        return {\n",
    "            'input_ids':inputs['input_ids'],\n",
    "            'attention_mask':inputs['attention_mask'],\n",
    "            'token_start': token_start\n",
    "            }\n",
    "\n",
    "    \n",
    "\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataset = TestDataset(CFG, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    1, 50759, 35872,   260, 77653,     2,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'token_start': tensor([-1,  1,  1,  1,  1, -1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0], dtype=torch.int32)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(testDataset,\n",
    "                              batch_size = CFG.batch_size,\n",
    "                              shuffle=False,\n",
    "                            #   collate_fn = collate_fn,\n",
    "                              num_workers = CFG.num_workers,\n",
    "                              pin_memory = True,\n",
    "                              drop_last=False)\n",
    "\n",
    "# for step, data in enumerate(test_loader):\n",
    "#     print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(module):\n",
    "    \"\"\"\n",
    "    Freezes module's parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    for parameter in module.parameters():\n",
    "        parameter.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NER_Model(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(NER_Model, self).__init__()\n",
    "\n",
    "        self.cfg = CFG\n",
    "        self.config = AutoConfig.from_pretrained(model_name)\n",
    "        self.config.hidden_dropout_prob = 0\n",
    "        self.config.attention_probs_dropout_prob = 0\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(model_name, config=self.config)\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.LayerNorm(self.config.hidden_size),\n",
    "            nn.Linear(self.config.hidden_size, self.cfg.target_size)\n",
    "        )\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids=None, targets=None, input_token_starts = None):\n",
    "        if token_type_ids:\n",
    "            transformer_out = self.model(ids, mask, token_type_ids)\n",
    "        else:\n",
    "            transformer_out = self.model(ids, mask)\n",
    "        \n",
    "        sequence_output = transformer_out[0] # shape : (batch,length,dimension)\n",
    "\n",
    "        # 去除[CLS]标签等位置，获得与label对齐的pre_label表示\n",
    "        # token_sequence_output = [layer[starts.nonzero().squeeze(1)]\n",
    "        #                           for layer, starts in zip(sequence_output, input_token_starts)]\n",
    "        \n",
    "        # 将sequence_output的pred_label维度padding到最大长度\n",
    "        # padded_sequence_output = pad_sequence(token_sequence_output, batch_first=True)\n",
    "        \n",
    "        logits = self.output(sequence_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NER_Model(\n",
       "  (model): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 1024, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (output): Sequential(\n",
       "    (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (1): Linear(in_features=1024, out_features=34, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# opensetid_model_path = \"C:/Users/Lab000/Desktop/2023_AI_CUP秋季/code/NER/巡迴課程資料集/FB3_13th_solution/AI_CUP_1/model\"\n",
    "model = NER_Model(CFG.model)\n",
    "\n",
    "state = torch.load(f\"./AI_CUP_3/infer_model/deberta-v3-large_full-data-2_gpt-5/{CFG.model.replace('/', '-')}_best.pth\",\n",
    "                                   map_location=torch.device('cpu'))['model']\n",
    "\n",
    "# state = model.load_state_dict(f\"./AI_CUP_3/infer_model/deberta-v3-large_full-data-2_gpt-5/{CFG.model.replace('/', '-')}_best.pth\")['model']\n",
    "\n",
    "# state = torch.load(f\"{opensetid_model_path}/{CFG.model.replace('/', '-')}_best.pth\",\n",
    "#                                    map_location=torch.device('cpu'))['model']\n",
    "    \n",
    "model.load_state_dict(state)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:08<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_tags = []\n",
    "\n",
    "for step, data in enumerate(tqdm(test_loader)):\n",
    "    model.eval()\n",
    "    ids = data['input_ids'].to(device, dtype=torch.long)\n",
    "    mask = data['attention_mask'].to(device, dtype=torch.long)\n",
    "    token_start = data['token_start'].to(device, dtype=torch.long)\n",
    "    with torch.no_grad():\n",
    "        logits = model(ids, mask)\n",
    "\n",
    "    token_position = token_start.gt(0)\n",
    "    dims = token_position.shape\n",
    "    \n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    token_position = token_position.detach().cpu().numpy()\n",
    "\n",
    "    for text_no in range(dims[0]):\n",
    "        text_tags = []\n",
    "        for pos in range(dims[1]):\n",
    "            if token_position[text_no,pos]:\n",
    "                \n",
    "                text_tags.append(id_to_label[np.argmax(logits[text_no, pos])])\n",
    "        # print(text_tags)\n",
    "        pred_tags.append(text_tags)\n",
    "\n",
    "    # active_loss = token_position.view(-1) == 1\n",
    "    # active_logits = logits.view(-1, CFG.target_size)[active_loss]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['MEDICALRECORD', 'MEDICALRECORD', 'MEDICALRECORD', 'MEDICALRECORD'],\n",
       " ['PATIENT', 'PATIENT', 'PATIENT', 'PATIENT'],\n",
       " ['IDNUM',\n",
       "  'IDNUM',\n",
       "  'IDNUM',\n",
       "  'IDNUM',\n",
       "  'PHI',\n",
       "  'IDNUM',\n",
       "  'IDNUM',\n",
       "  'IDNUM',\n",
       "  'IDNUM'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'DATE',\n",
       "  'DATE',\n",
       "  'DATE',\n",
       "  'DATE',\n",
       "  'DATE',\n",
       "  'DATE',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI',\n",
       "  'DOCTOR',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'DATE',\n",
       "  'DATE',\n",
       "  'DATE',\n",
       "  'DATE',\n",
       "  'DATE',\n",
       "  'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'DOCTOR',\n",
       "  'DOCTOR',\n",
       "  'DOCTOR',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'IDNUM',\n",
       "  'IDNUM',\n",
       "  'IDNUM',\n",
       "  'IDNUM',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'IDNUM', 'IDNUM', 'IDNUM', 'IDNUM', 'IDNUM'],\n",
       " ['MEDICALRECORD',\n",
       "  'MEDICALRECORD',\n",
       "  'MEDICALRECORD',\n",
       "  'MEDICALRECORD',\n",
       "  'MEDICALRECORD',\n",
       "  'MEDICALRECORD'],\n",
       " ['PATIENT', 'PATIENT', 'PATIENT', 'PATIENT', 'PATIENT', 'PATIENT'],\n",
       " ['PHI', 'PHI', 'PHI', 'IDNUM', 'IDNUM', 'IDNUM', 'IDNUM'],\n",
       " ['STREET', 'STREET'],\n",
       " ['CITY', 'CITY', 'STATE', 'ZIP', 'ZIP'],\n",
       " ['PHI', 'PHI', 'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'DATE',\n",
       "  'DATE',\n",
       "  'DATE',\n",
       "  'DATE',\n",
       "  'DATE'],\n",
       " ['PHI', 'PHI', 'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'DATE',\n",
       "  'DATE',\n",
       "  'DATE',\n",
       "  'DATE',\n",
       "  'DATE',\n",
       "  'DATE',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'DEPARTMENT',\n",
       "  'DEPARTMENT',\n",
       "  'DEPARTMENT',\n",
       "  'DEPARTMENT',\n",
       "  'PHI',\n",
       "  'HOSPITAL',\n",
       "  'HOSPITAL',\n",
       "  'HOSPITAL'],\n",
       " ['PHI', 'DOCTOR', 'DOCTOR', 'DOCTOR'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI', 'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI',\n",
       "  'DOCTOR',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'DATE',\n",
       "  'DATE',\n",
       "  'DATE',\n",
       "  'DATE',\n",
       "  'DATE',\n",
       "  'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'DOCTOR',\n",
       "  'DOCTOR',\n",
       "  'DOCTOR',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI'],\n",
       " ['PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI'],\n",
       " ['PHI', 'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI'],\n",
       " ['PHI', 'PHI', 'PHI'],\n",
       " ['PHI'],\n",
       " ['PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI'],\n",
       " ['PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI'],\n",
       " ['PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI'],\n",
       " ['PHI', 'PHI', 'PHI'],\n",
       " ['PHI'],\n",
       " ['PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI', 'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI'],\n",
       " ['PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI', 'PHI'],\n",
       " ['PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI',\n",
       "  'PHI']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文章總數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文章總數: 2\n",
      "文章總數: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"文章總數:\",test_df[\"fid\"].unique().shape[0])\n",
    "print(\"文章總數:\",len(test_df[(test_df[\"idx\"]==0)]) + len(test_df[(test_df[\"idx\"]==1)]) + len(test_df[(test_df[\"idx\"]==2)])) #文章第一句起始為0、1或2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 寫入answer.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,j in enumerate(pred_tags):\n",
    "    pred_tags[index] = [label_to_id [k]for k in pred_tags[index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'PATIENT',\n",
       " 1: 'DOCTOR',\n",
       " 2: 'USERNAME',\n",
       " 3: 'PROFESSION',\n",
       " 4: 'ROOM',\n",
       " 5: 'DEPARTMENT',\n",
       " 6: 'HOSPITAL',\n",
       " 7: 'ORGANIZATION',\n",
       " 8: 'STREET',\n",
       " 9: 'CITY',\n",
       " 10: 'STATE',\n",
       " 11: 'COUNTRY',\n",
       " 12: 'ZIP',\n",
       " 13: 'LOCATION-OTHER',\n",
       " 14: 'AGE',\n",
       " 15: 'DATE',\n",
       " 16: 'TIME',\n",
       " 17: 'DURATION',\n",
       " 18: 'SET',\n",
       " 19: 'PHONE',\n",
       " 20: 'FAX',\n",
       " 21: 'EMAIL',\n",
       " 22: 'URL',\n",
       " 23: 'IPADDR',\n",
       " 24: 'SSN',\n",
       " 25: 'MEDICALRECORD',\n",
       " 26: 'HEALTHPLAN',\n",
       " 27: 'ACCOUNT',\n",
       " 28: 'LICENSE',\n",
       " 29: 'VECHICLE',\n",
       " 30: 'DEVICE',\n",
       " 31: 'BIOID',\n",
       " 32: 'IDNUM',\n",
       " 33: 'PHI'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[25, 25, 25, 25],\n",
       " [0, 0, 0, 0],\n",
       " [32, 32, 32, 32, 33, 32, 32, 32, 32],\n",
       " [33, 33, 33, 15, 15, 15, 15, 15, 15, 33, 33, 33],\n",
       " [33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33],\n",
       " [33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33],\n",
       " [33, 1, 33, 33, 15, 15, 15, 15, 15, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33, 1, 1, 1, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33, 33],\n",
       " [33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33],\n",
       " [33],\n",
       " [33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  32,\n",
       "  33,\n",
       "  33],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 32, 32, 32, 32, 32],\n",
       " [25, 25, 25, 25, 25, 25],\n",
       " [0, 0, 0, 0, 0, 0],\n",
       " [33, 33, 33, 32, 32, 32, 32],\n",
       " [8, 8],\n",
       " [9, 9, 10, 12, 12],\n",
       " [33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 15, 15, 15, 15, 15],\n",
       " [33, 33, 33],\n",
       " [33, 33, 15, 15, 15, 15, 15, 15, 33, 33, 33, 33],\n",
       " [33, 33, 5, 5, 5, 5, 33, 6, 6, 6],\n",
       " [33, 1, 1, 1],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33],\n",
       " [33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33],\n",
       " [33, 33],\n",
       " [33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33],\n",
       " [33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33],\n",
       " [33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33],\n",
       " [33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33, 33, 33],\n",
       " [33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33],\n",
       " [33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33],\n",
       " [33, 1, 33, 33, 15, 15, 15, 15, 15, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33, 1, 1, 1, 33, 33],\n",
       " [33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33],\n",
       " [33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33],\n",
       " [33, 33, 33, 33, 33, 33],\n",
       " [33, 33],\n",
       " [33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33],\n",
       " [33, 33],\n",
       " [33],\n",
       " [33, 33, 33, 33],\n",
       " [33, 33, 33, 33],\n",
       " [33],\n",
       " [33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33],\n",
       " [33],\n",
       " [33, 33, 33],\n",
       " [33],\n",
       " [33, 33],\n",
       " [33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33],\n",
       " [33, 33],\n",
       " [33],\n",
       " [33, 33, 33, 33],\n",
       " [33, 33, 33, 33],\n",
       " [33],\n",
       " [33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33],\n",
       " [33],\n",
       " [33, 33, 33],\n",
       " [33],\n",
       " [33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33],\n",
       " [33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33],\n",
       " [33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33],\n",
       " [33, 33, 33, 33, 33, 33, 33],\n",
       " [33, 33],\n",
       " [33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33],\n",
       " [33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33],\n",
       " [33, 33, 33, 33, 33, 33, 33],\n",
       " [33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33,\n",
       "  33]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = [0 if j==33 else j for j in pred_tags[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_continuous_and_non_continuous_numbers_and_indices(input_list):\n",
    "    result = []\n",
    "    current_number = input_list[0]\n",
    "    start_index = 0\n",
    "\n",
    "    for i in range(1, len(input_list)):\n",
    "        if input_list[i] != current_number:\n",
    "            end_index = i - 1\n",
    "            result.append((current_number, (start_index, end_index)))\n",
    "            current_number = input_list[i]\n",
    "            start_index = i\n",
    "\n",
    "    # 處理最後一個連續或非連續數字序列\n",
    "    end_index = len(input_list) - 1\n",
    "    result.append((current_number, (start_index, end_index)))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fid</th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1097</td>\n",
       "      <td>1</td>\n",
       "      <td>433475.RDC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1097</td>\n",
       "      <td>12</td>\n",
       "      <td>Timmins, ELDEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1097</td>\n",
       "      <td>27</td>\n",
       "      <td>43J47561,43J47561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1097</td>\n",
       "      <td>46</td>\n",
       "      <td>Last edited : 7/9/2063  Page: 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1097</td>\n",
       "      <td>78</td>\n",
       "      <td>CLINICAL:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1097</td>\n",
       "      <td>88</td>\n",
       "      <td>Metastatic cancer ?colorectal primary.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1097</td>\n",
       "      <td>128</td>\n",
       "      <td>MACROSCOPIC:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1097</td>\n",
       "      <td>141</td>\n",
       "      <td>Specimen labelled \"Omentum secondary\", consist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1097</td>\n",
       "      <td>230</td>\n",
       "      <td>On sectioning there are multiple fibrotic whit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1097</td>\n",
       "      <td>312</td>\n",
       "      <td>Blocks: 1 to 5 - representative sections from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1097</td>\n",
       "      <td>371</td>\n",
       "      <td>Block 3 reserved block.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1097</td>\n",
       "      <td>395</td>\n",
       "      <td>(IC/vo 5.9.63)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1097</td>\n",
       "      <td>410</td>\n",
       "      <td>MICROSCOPIC (Reported by Dr L Bonnot):</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1097</td>\n",
       "      <td>449</td>\n",
       "      <td>Sections show omental fat with metastatic aden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1097</td>\n",
       "      <td>507</td>\n",
       "      <td>The lesion has a complex glandular architectur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1097</td>\n",
       "      <td>624</td>\n",
       "      <td>The tumour cells are positive for CK7 and CK20.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1097</td>\n",
       "      <td>672</td>\n",
       "      <td>TTF-1 is negative.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1097</td>\n",
       "      <td>691</td>\n",
       "      <td>COMMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1097</td>\n",
       "      <td>699</td>\n",
       "      <td>AlthoughCK7 staining is unusual for a colorect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1097</td>\n",
       "      <td>845</td>\n",
       "      <td>The overall findings would favour a colorectal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1097</td>\n",
       "      <td>902</td>\n",
       "      <td>DIAGNOSIS:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1097</td>\n",
       "      <td>913</td>\n",
       "      <td>Omentum: metastatic adenocarcinoma, favour col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1135</td>\n",
       "      <td>0</td>\n",
       "      <td>Episode No:  23F340166Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1135</td>\n",
       "      <td>24</td>\n",
       "      <td>2323401.RRQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1135</td>\n",
       "      <td>37</td>\n",
       "      <td>Jourdan, WILLIEMAE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1135</td>\n",
       "      <td>57</td>\n",
       "      <td>Lab No:  23F34016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1135</td>\n",
       "      <td>75</td>\n",
       "      <td>Redacre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1135</td>\n",
       "      <td>83</td>\n",
       "      <td>COWRA  WA  6021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1135</td>\n",
       "      <td>99</td>\n",
       "      <td>Specimen: Tissue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1135</td>\n",
       "      <td>116</td>\n",
       "      <td>D.O.B:  3/6/1989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1135</td>\n",
       "      <td>133</td>\n",
       "      <td>Sex:  F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1135</td>\n",
       "      <td>141</td>\n",
       "      <td>Collected: 07/08/2062 at 12:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1135</td>\n",
       "      <td>172</td>\n",
       "      <td>Location:  ENVOI Pathology-TEMORA HOSPITAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1135</td>\n",
       "      <td>215</td>\n",
       "      <td>DR ANTHONY ROCKHOLD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1135</td>\n",
       "      <td>235</td>\n",
       "      <td>Distribution:   FILE-COPY,   NSW-CANCER-REGISTRY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1135</td>\n",
       "      <td>284</td>\n",
       "      <td>CLINICAL:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1135</td>\n",
       "      <td>294</td>\n",
       "      <td>Large bowel obstruction.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1135</td>\n",
       "      <td>320</td>\n",
       "      <td>Extended right hemicolectomy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1135</td>\n",
       "      <td>351</td>\n",
       "      <td>Transverse colon polyp.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1135</td>\n",
       "      <td>376</td>\n",
       "      <td>Diarrhoea for  8/9.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1135</td>\n",
       "      <td>397</td>\n",
       "      <td>RX with metronidazole.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1135</td>\n",
       "      <td>422</td>\n",
       "      <td>MACROSCOPIC:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1135</td>\n",
       "      <td>435</td>\n",
       "      <td>A.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1135</td>\n",
       "      <td>439</td>\n",
       "      <td>Specimen labelled \"Right hemicolectomy\" consis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1135</td>\n",
       "      <td>606</td>\n",
       "      <td>60mm from the distal end there is an obstructi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1135</td>\n",
       "      <td>758</td>\n",
       "      <td>The cut surface reveals a cream tumour appeari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1135</td>\n",
       "      <td>866</td>\n",
       "      <td>Near the ileocaecal valve in thecaecum there i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1135</td>\n",
       "      <td>942</td>\n",
       "      <td>200mm from the distal end there is a further s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1135</td>\n",
       "      <td>1013</td>\n",
       "      <td>A further small polyp 5mm is identified.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1135</td>\n",
       "      <td>1055</td>\n",
       "      <td>All polyps appear confined to the mucosa.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1135</td>\n",
       "      <td>1098</td>\n",
       "      <td>The remainder of the right colon appears dilat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1135</td>\n",
       "      <td>1163</td>\n",
       "      <td>A small piece of mesentery 60 x 35 x 8mm in in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1135</td>\n",
       "      <td>1218</td>\n",
       "      <td>Multiple lymph nodes are identified within the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1135</td>\n",
       "      <td>1283</td>\n",
       "      <td>Blocks: 1 - margins; 2 - tumour to uninvolved ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1135</td>\n",
       "      <td>1530</td>\n",
       "      <td>B.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1135</td>\n",
       "      <td>1534</td>\n",
       "      <td>Specimen labelled \"Bowel polyp\" consists of a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1135</td>\n",
       "      <td>1592</td>\n",
       "      <td>BAE 1 block.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1135</td>\n",
       "      <td>1605</td>\n",
       "      <td>(FA/ta 8/8/62)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>1135</td>\n",
       "      <td>1621</td>\n",
       "      <td>MICROSCOPIC (reported by Dr F Serpe):</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>1135</td>\n",
       "      <td>1659</td>\n",
       "      <td>A &amp; B.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1135</td>\n",
       "      <td>1667</td>\n",
       "      <td>Sections show two foci of adenocarcinoma.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>1135</td>\n",
       "      <td>1710</td>\n",
       "      <td>The first designated tumour 1 is situated with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>1135</td>\n",
       "      <td>1842</td>\n",
       "      <td>The features are as follows:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1135</td>\n",
       "      <td>1871</td>\n",
       "      <td>Tumour 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1135</td>\n",
       "      <td>1881</td>\n",
       "      <td>Macroscopic Description</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1135</td>\n",
       "      <td>1906</td>\n",
       "      <td>Site of tumour:  Ascending colon.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1135</td>\n",
       "      <td>1942</td>\n",
       "      <td>Maximum tumour diameter:  15mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1135</td>\n",
       "      <td>1973</td>\n",
       "      <td>Distance of tumour to nearer cut end:  60mm (d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1135</td>\n",
       "      <td>2026</td>\n",
       "      <td>Tumour perforation:   No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1135</td>\n",
       "      <td>2051</td>\n",
       "      <td>Microscopic Description</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1135</td>\n",
       "      <td>2076</td>\n",
       "      <td>Type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1135</td>\n",
       "      <td>2081</td>\n",
       "      <td>Adenocarcinoma NOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1135</td>\n",
       "      <td>2100</td>\n",
       "      <td>Differentiation by predominant area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1135</td>\n",
       "      <td>2136</td>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1135</td>\n",
       "      <td>2145</td>\n",
       "      <td>Local invasion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1135</td>\n",
       "      <td>2160</td>\n",
       "      <td>pT4a Tumour penetrates the surface of the visc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1135</td>\n",
       "      <td>2222</td>\n",
       "      <td>Tumour involvement of margins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1135</td>\n",
       "      <td>2252</td>\n",
       "      <td>Cut ends:  Not involved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1135</td>\n",
       "      <td>2276</td>\n",
       "      <td>Lymphovascular invasion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1135</td>\n",
       "      <td>2300</td>\n",
       "      <td>Absent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1135</td>\n",
       "      <td>2307</td>\n",
       "      <td>Perineural invasion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1135</td>\n",
       "      <td>2327</td>\n",
       "      <td>Absent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1135</td>\n",
       "      <td>2334</td>\n",
       "      <td>Tumour 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1135</td>\n",
       "      <td>2344</td>\n",
       "      <td>Macroscopic Description</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1135</td>\n",
       "      <td>2369</td>\n",
       "      <td>Site of tumour:  Caecum.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1135</td>\n",
       "      <td>2396</td>\n",
       "      <td>Maximum tumour diameter:  15mm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1135</td>\n",
       "      <td>2427</td>\n",
       "      <td>Distance of tumour to nearer cut end:  40mm (p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1135</td>\n",
       "      <td>2482</td>\n",
       "      <td>Tumour perforation:   No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1135</td>\n",
       "      <td>2507</td>\n",
       "      <td>Microscopic Description</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1135</td>\n",
       "      <td>2532</td>\n",
       "      <td>Type</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1135</td>\n",
       "      <td>2537</td>\n",
       "      <td>Adenocarcinoma NOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1135</td>\n",
       "      <td>2556</td>\n",
       "      <td>Differentiation by predominant area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1135</td>\n",
       "      <td>2592</td>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1135</td>\n",
       "      <td>2602</td>\n",
       "      <td>Local invasion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1135</td>\n",
       "      <td>2617</td>\n",
       "      <td>pT2 Invasion into muscularis propria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1135</td>\n",
       "      <td>2654</td>\n",
       "      <td>Tumour involvement of margins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1135</td>\n",
       "      <td>2684</td>\n",
       "      <td>Cut ends:  Not involved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1135</td>\n",
       "      <td>2708</td>\n",
       "      <td>Lymphovascular invasion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1135</td>\n",
       "      <td>2732</td>\n",
       "      <td>Absent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1135</td>\n",
       "      <td>2739</td>\n",
       "      <td>Perineural invasion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1135</td>\n",
       "      <td>2759</td>\n",
       "      <td>Absent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1135</td>\n",
       "      <td>2768</td>\n",
       "      <td>Lymph nodes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1135</td>\n",
       "      <td>2780</td>\n",
       "      <td>Number of lymph nodes present:  11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>1135</td>\n",
       "      <td>2816</td>\n",
       "      <td>Number of lymph nodes involved:  0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1135</td>\n",
       "      <td>2852</td>\n",
       "      <td>pN0 No regional LN metastasis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1135</td>\n",
       "      <td>2882</td>\n",
       "      <td>Histologically confirmed distant metastases</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1135</td>\n",
       "      <td>2926</td>\n",
       "      <td>pMX Cannot be assessed histologically</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>1135</td>\n",
       "      <td>2964</td>\n",
       "      <td>Background abnormalities/other comments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>1135</td>\n",
       "      <td>3004</td>\n",
       "      <td>Sessile serrated adenoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>1135</td>\n",
       "      <td>3029</td>\n",
       "      <td>Serrated adenoma with low grade epithelial dys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1135</td>\n",
       "      <td>3095</td>\n",
       "      <td>Residual tumour status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>1135</td>\n",
       "      <td>3118</td>\n",
       "      <td>R0 No residual tumour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1135</td>\n",
       "      <td>3140</td>\n",
       "      <td>Mismatch Repair Deficiency (MMRD) Status</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1135</td>\n",
       "      <td>3182</td>\n",
       "      <td>Tumour 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1135</td>\n",
       "      <td>3192</td>\n",
       "      <td>MLH1  Preserved nuclear staining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>1135</td>\n",
       "      <td>3225</td>\n",
       "      <td>PMS2  Preserved nuclear staining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1135</td>\n",
       "      <td>3258</td>\n",
       "      <td>MSH2  Preserved nuclear staining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1135</td>\n",
       "      <td>3291</td>\n",
       "      <td>MSH6  Preserved nuclear staining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1135</td>\n",
       "      <td>3324</td>\n",
       "      <td>Tumour 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>1135</td>\n",
       "      <td>3333</td>\n",
       "      <td>MLH1  Equivocal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1135</td>\n",
       "      <td>3349</td>\n",
       "      <td>PMS2  Loss of nuclear staining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1135</td>\n",
       "      <td>3380</td>\n",
       "      <td>MSH2  Equivocal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>1135</td>\n",
       "      <td>3396</td>\n",
       "      <td>MSH6  Loss of nuclear staining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1135</td>\n",
       "      <td>3427</td>\n",
       "      <td>Comment:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1135</td>\n",
       "      <td>3436</td>\n",
       "      <td>Absence (loss) of nuclear staining for any of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1135</td>\n",
       "      <td>3703</td>\n",
       "      <td>Preserved nuclear staining of a carcinoma for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1135</td>\n",
       "      <td>3843</td>\n",
       "      <td>Summary - TNM 7th Edition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>1135</td>\n",
       "      <td>3869</td>\n",
       "      <td>pT4a (tumour 1)  pT2 (tumour 2)  pN0   pMX    R0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fid   idx                                               text\n",
       "0    1097     1                                         433475.RDC\n",
       "1    1097    12                                     Timmins, ELDEN\n",
       "2    1097    27                                  43J47561,43J47561\n",
       "3    1097    46                    Last edited : 7/9/2063  Page: 2\n",
       "4    1097    78                                          CLINICAL:\n",
       "5    1097    88             Metastatic cancer ?colorectal primary.\n",
       "6    1097   128                                       MACROSCOPIC:\n",
       "7    1097   141  Specimen labelled \"Omentum secondary\", consist...\n",
       "8    1097   230  On sectioning there are multiple fibrotic whit...\n",
       "9    1097   312  Blocks: 1 to 5 - representative sections from ...\n",
       "10   1097   371                            Block 3 reserved block.\n",
       "11   1097   395                                     (IC/vo 5.9.63)\n",
       "12   1097   410             MICROSCOPIC (Reported by Dr L Bonnot):\n",
       "13   1097   449  Sections show omental fat with metastatic aden...\n",
       "14   1097   507  The lesion has a complex glandular architectur...\n",
       "15   1097   624    The tumour cells are positive for CK7 and CK20.\n",
       "16   1097   672                                 TTF-1 is negative.\n",
       "17   1097   691                                            COMMENT\n",
       "18   1097   699  AlthoughCK7 staining is unusual for a colorect...\n",
       "19   1097   845  The overall findings would favour a colorectal...\n",
       "20   1097   902                                         DIAGNOSIS:\n",
       "21   1097   913  Omentum: metastatic adenocarcinoma, favour col...\n",
       "22   1135     0                            Episode No:  23F340166Q\n",
       "23   1135    24                                        2323401.RRQ\n",
       "24   1135    37                                 Jourdan, WILLIEMAE\n",
       "25   1135    57                                  Lab No:  23F34016\n",
       "26   1135    75                                            Redacre\n",
       "27   1135    83                                    COWRA  WA  6021\n",
       "28   1135    99                                   Specimen: Tissue\n",
       "29   1135   116                                   D.O.B:  3/6/1989\n",
       "30   1135   133                                            Sex:  F\n",
       "31   1135   141                     Collected: 07/08/2062 at 12:20\n",
       "32   1135   172         Location:  ENVOI Pathology-TEMORA HOSPITAL\n",
       "33   1135   215                                DR ANTHONY ROCKHOLD\n",
       "34   1135   235   Distribution:   FILE-COPY,   NSW-CANCER-REGISTRY\n",
       "35   1135   284                                          CLINICAL:\n",
       "36   1135   294                           Large bowel obstruction.\n",
       "37   1135   320                      Extended right hemicolectomy.\n",
       "38   1135   351                            Transverse colon polyp.\n",
       "39   1135   376                                Diarrhoea for  8/9.\n",
       "40   1135   397                             RX with metronidazole.\n",
       "41   1135   422                                       MACROSCOPIC:\n",
       "42   1135   435                                                 A.\n",
       "43   1135   439  Specimen labelled \"Right hemicolectomy\" consis...\n",
       "44   1135   606  60mm from the distal end there is an obstructi...\n",
       "45   1135   758  The cut surface reveals a cream tumour appeari...\n",
       "46   1135   866  Near the ileocaecal valve in thecaecum there i...\n",
       "47   1135   942  200mm from the distal end there is a further s...\n",
       "48   1135  1013           A further small polyp 5mm is identified.\n",
       "49   1135  1055          All polyps appear confined to the mucosa.\n",
       "50   1135  1098  The remainder of the right colon appears dilat...\n",
       "51   1135  1163  A small piece of mesentery 60 x 35 x 8mm in in...\n",
       "52   1135  1218  Multiple lymph nodes are identified within the...\n",
       "53   1135  1283  Blocks: 1 - margins; 2 - tumour to uninvolved ...\n",
       "54   1135  1530                                                 B.\n",
       "55   1135  1534  Specimen labelled \"Bowel polyp\" consists of a ...\n",
       "56   1135  1592                                       BAE 1 block.\n",
       "57   1135  1605                                     (FA/ta 8/8/62)\n",
       "58   1135  1621              MICROSCOPIC (reported by Dr F Serpe):\n",
       "59   1135  1659                                             A & B.\n",
       "60   1135  1667          Sections show two foci of adenocarcinoma.\n",
       "61   1135  1710  The first designated tumour 1 is situated with...\n",
       "62   1135  1842                       The features are as follows:\n",
       "63   1135  1871                                           Tumour 1\n",
       "64   1135  1881                            Macroscopic Description\n",
       "65   1135  1906                  Site of tumour:  Ascending colon.\n",
       "66   1135  1942                     Maximum tumour diameter:  15mm\n",
       "67   1135  1973  Distance of tumour to nearer cut end:  60mm (d...\n",
       "68   1135  2026                           Tumour perforation:   No\n",
       "69   1135  2051                            Microscopic Description\n",
       "70   1135  2076                                               Type\n",
       "71   1135  2081                                 Adenocarcinoma NOS\n",
       "72   1135  2100                Differentiation by predominant area\n",
       "73   1135  2136                                           Moderate\n",
       "74   1135  2145                                     Local invasion\n",
       "75   1135  2160  pT4a Tumour penetrates the surface of the visc...\n",
       "76   1135  2222                      Tumour involvement of margins\n",
       "77   1135  2252                            Cut ends:  Not involved\n",
       "78   1135  2276                            Lymphovascular invasion\n",
       "79   1135  2300                                             Absent\n",
       "80   1135  2307                                Perineural invasion\n",
       "81   1135  2327                                             Absent\n",
       "82   1135  2334                                           Tumour 2\n",
       "83   1135  2344                            Macroscopic Description\n",
       "84   1135  2369                           Site of tumour:  Caecum.\n",
       "85   1135  2396                     Maximum tumour diameter:  15mm\n",
       "86   1135  2427  Distance of tumour to nearer cut end:  40mm (p...\n",
       "87   1135  2482                           Tumour perforation:   No\n",
       "88   1135  2507                            Microscopic Description\n",
       "89   1135  2532                                               Type\n",
       "90   1135  2537                                 Adenocarcinoma NOS\n",
       "91   1135  2556                Differentiation by predominant area\n",
       "92   1135  2592                                           Moderate\n",
       "93   1135  2602                                     Local invasion\n",
       "94   1135  2617               pT2 Invasion into muscularis propria\n",
       "95   1135  2654                      Tumour involvement of margins\n",
       "96   1135  2684                            Cut ends:  Not involved\n",
       "97   1135  2708                            Lymphovascular invasion\n",
       "98   1135  2732                                             Absent\n",
       "99   1135  2739                                Perineural invasion\n",
       "100  1135  2759                                             Absent\n",
       "101  1135  2768                                        Lymph nodes\n",
       "102  1135  2780                 Number of lymph nodes present:  11\n",
       "103  1135  2816                 Number of lymph nodes involved:  0\n",
       "104  1135  2852                      pN0 No regional LN metastasis\n",
       "105  1135  2882        Histologically confirmed distant metastases\n",
       "106  1135  2926              pMX Cannot be assessed histologically\n",
       "107  1135  2964            Background abnormalities/other comments\n",
       "108  1135  3004                           Sessile serrated adenoma\n",
       "109  1135  3029  Serrated adenoma with low grade epithelial dys...\n",
       "110  1135  3095                             Residual tumour status\n",
       "111  1135  3118                              R0 No residual tumour\n",
       "112  1135  3140           Mismatch Repair Deficiency (MMRD) Status\n",
       "113  1135  3182                                           Tumour 1\n",
       "114  1135  3192                   MLH1  Preserved nuclear staining\n",
       "115  1135  3225                   PMS2  Preserved nuclear staining\n",
       "116  1135  3258                   MSH2  Preserved nuclear staining\n",
       "117  1135  3291                   MSH6  Preserved nuclear staining\n",
       "118  1135  3324                                           Tumour 2\n",
       "119  1135  3333                                    MLH1  Equivocal\n",
       "120  1135  3349                     PMS2  Loss of nuclear staining\n",
       "121  1135  3380                                    MSH2  Equivocal\n",
       "122  1135  3396                     MSH6  Loss of nuclear staining\n",
       "123  1135  3427                                           Comment:\n",
       "124  1135  3436  Absence (loss) of nuclear staining for any of ...\n",
       "125  1135  3703  Preserved nuclear staining of a carcinoma for ...\n",
       "126  1135  3843                          Summary - TNM 7th Edition\n",
       "127  1135  3869   pT4a (tumour 1)  pT2 (tumour 2)  pN0   pMX    R0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# post-process-ver-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_label_name_list = [\"IDNUM\",\"MEDICALRECORD\",\"PATIENT\",\"CITY\",\"STATE\",\"ZIP\",\"DEPARTMENT\",\n",
    "                  \"HOSPITAL\",\"DOCTOR\",\"STREET\",\"ORGANIZATION\",\"AGE\",\n",
    "                  \"DATE\",\"TIME\",\"PHONE\"]\n",
    "pre_label_id_list = [label_to_id[label_name] for label_name in pre_label_name_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:00<00:00, 8533.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1097\tMEDICALRECORD\t1\t11\t433475.RDC\n",
      "1097\tPATIENT\t12\t26\tTimmins, ELDEN\n",
      "1097\tIDNUM\t27\t35\t43J47561\n",
      "1097\tIDNUM\t27\t35\t43J47561\n",
      "1097\tDATE\t60\t68\t7/9/2063\n",
      "1097\tDOCTOR\t396\t398\tIC\n",
      "1097\tDATE\t402\t408\t5.9.63\n",
      "1097\tDOCTOR\t438\t446\tL Bonnot\n",
      "1097\tIDNUM\t835\t842\t43J4756\n",
      "1135\tIDNUM\t13\t23\t23F340166Q\n",
      "1135\tMEDICALRECORD\t24\t35\t2323401.RRQ\n",
      "1135\tPATIENT\t37\t55\tJourdan, WILLIEMAE\n",
      "1135\tIDNUM\t66\t74\t23F34016\n",
      "1135\tSTREET\t75\t82\tRedacre\n",
      "1135\tCITY\t83\t88\tCOWRA\n",
      "1135\tSTATE\t90\t92\tWA\n",
      "1135\tZIP\t94\t98\t6021\n",
      "1135\tDATE\t124\t132\t3/6/1989\n",
      "1135\tDATE\t152\t162\t07/08/2062\n",
      "1135\tDEPARTMENT\t183\t198\tENVOI Pathology\n",
      "1135\tHOSPITAL\t199\t214\tTEMORA HOSPITAL\n",
      "1135\tDOCTOR\t218\t234\tANTHONY ROCKHOLD\n",
      "1135\tDOCTOR\t1606\t1608\tFA\n",
      "1135\tDATE\t1612\t1618\t8/8/62\n",
      "1135\tDOCTOR\t1649\t1656\tF Serpe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "lengths = []\n",
    "tk0 = tqdm(test_df['text'].fillna(\"\").values, total=len(test_df))\n",
    "\n",
    "final_pre_text = []\n",
    "\n",
    "\n",
    "for index,text in enumerate(tk0):\n",
    "    \n",
    "    \n",
    "\n",
    "    fid = test_df.loc[index][\"fid\"]\n",
    "    idx = test_df.loc[index][\"idx\"]\n",
    "\n",
    "    # token_list = CFG.tokenizer(text, add_special_tokens=False)['input_ids']\n",
    "    \n",
    "    encoded = CFG.tokenizer(text,\n",
    "                            add_special_tokens=False,\n",
    "                            return_offsets_mapping=True)\n",
    "    \n",
    "    token_list = encoded['input_ids']\n",
    "    offset_mapping_list = encoded['offset_mapping']\n",
    "    \n",
    "\n",
    "    WithoutNULL = [-1 if j==33 else j for j in pred_tags[index]]\n",
    "\n",
    "    pre_text_label_and_index = find_continuous_and_non_continuous_numbers_and_indices(WithoutNULL)\n",
    "    \n",
    "    for position, pre_word_label_and_index in enumerate(pre_text_label_and_index) :\n",
    "        pre_label = pre_word_label_and_index[0]\n",
    "        pre_word_start_end = pre_word_label_and_index[1]\n",
    "\n",
    "        if pre_label != -1 and pre_label in pre_label_id_list:\n",
    "        \n",
    "            start = pre_word_start_end[0]\n",
    "            end = pre_word_start_end[1]\n",
    "            \n",
    "            pre_word_text = CFG.tokenizer.decode(token_list[start:end+1])\n",
    "\n",
    "            if pre_word_text == '':# [507] token decode 為 ''，如果預測為PHI，寫進答案會Submission Error\n",
    "                continue\n",
    "\n",
    "            offset_idx = text.find(pre_word_text)\n",
    "\n",
    "            ########################################################################## \n",
    "            if pre_label == 13: # 針對LOCATION-OTHER，Decode時會少一個空白的處理\n",
    "                white_space_position = []\n",
    "                for i, offset_mapping in enumerate(offset_mapping_list):\n",
    "                    if i == 0:\n",
    "                        continue\n",
    "                    previous_offset_start = offset_mapping_list[i-1][0]\n",
    "                    previous_offset_end = offset_mapping_list[i-1][1]\n",
    "                    current_offset_start = offset_mapping_list[i][0]\n",
    "                    current_offset_end = offset_mapping_list[i][1]\n",
    "                    if previous_offset_end!=current_offset_start:\n",
    "                        white_space_position.append(previous_offset_end)\n",
    "\n",
    "                if len(white_space_position)!=0:\n",
    "                    for position in white_space_position:\n",
    "                        pre_word_text = pre_word_text[:position] + ' ' + pre_word_text[position:]\n",
    "                \n",
    "                offset_idx = text.find(pre_word_text)\n",
    "            ##########################################################################\n",
    "            \n",
    "                \n",
    "            print(f\"{fid}\\t{id_to_label[pre_label]}\\t{idx+offset_idx}\\t{idx+offset_idx+len(pre_word_text)}\\t{pre_word_text}\")\n",
    "\n",
    "            final_pre_text.append(f\"{fid}\\t{id_to_label[pre_label]}\\t{idx+offset_idx}\\t{idx+offset_idx+len(pre_word_text)}\\t{pre_word_text}\")\n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./DemoAnswer/NER/NER_answer.txt','w', encoding='utf-8') as f:\n",
    "        for final_pre in final_pre_text:\n",
    "            f.write(final_pre)\n",
    "            f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_CUP_Fall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
